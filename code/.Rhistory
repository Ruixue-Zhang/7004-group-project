fit <- lm(Credit.amount ~ Age,data = df.nomis)
summary(fit)
plot(df.nomis$Age,df.nomis$Credit.amount)
abline(fit)
fit2 <- lm(Credit.amount ~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data = df.nomis)
summary(fit2)
# ridge regression
set.seed(42)
model_ridge <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="ridge", metric="RMSE")
model_ridge
set.seed(42)
model_lars <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="lars", metric="RMSE")
model_lars
# Make predictions on the testing dataset
x_test_reg <- df.nomis[,1:6 ]
y_test_reg <- df.nomis[, 7]
prediction_lars <- predict(model_lars,x_test_reg)
RMSE(prediction_lars, y_test_reg)
# decision tree
colnames(df.nomis)
form_reg <- as.formula(
paste0('Credit.amount~',
paste(colnames(train)[1:6],collapse="+")
)
)
form_reg
# training model
set.seed(42)
fit_dt_reg <- rpart(
form_reg,#formula
data = train,
method = "anova",# regression
control = rpart.control(cp = 0.005)
)
# Original regression tree
fit_dt_reg
# Complexity correlation
printcp(fit_dt_reg)
plotcp(fit_dt_reg)
# After the cut branches
fit_dt_reg_pruned <- prune(fit_dt_reg,cp =0.024)
print(fit_dt_reg_pruned)
# Variable importance value
fit_dt_reg_pruned$variable.importance
# Variable importance diagram
varimpdata <-
data.frame(importance=fit_dt_reg_pruned$variable.importance)
ggplot(varimpdata,
aes(x = as_factor(rownames(varimpdata)),y=importance))+
geom_col()+
labs(x = 'variables')+
theme_classic()
# tree diagram
prp(fit_dt_reg_pruned,
type=1,
extra = 101,
fallen.leaves =TRUE,
main="Decision Tree")
# predicted
# The training set predicts the results
trainpred <- predit(fit_dt_reg_pruned,newdata=train)
# predicted
# The training set predicts the results
trainpred <- predict(fit_dt_reg_pruned,newdata=train)
# Graph the training set predicted results
plot(x=train$Credit.amount,
y=trainpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'training set')
trainlinmod <- lm(trainpred~train$Credit.amount)
abline(trainlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Test set prediction error index
defaultSummary(data.frame(obs= test$Credit.amount,pred=testpred))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'training set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'test set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# fold cross validation
train_control <- trainControl(method="cv", number=10)
fit <- lm(Credit.amount ~ Age,data = df.nomis)
summary(fit)
skim(df.nomis)
# Check the missing data
plot_missing(df.nomis)
class(df.nomis)
head(df.nomis)
# feature engineering
# Train/Test split for Regression
set.seed(42)
train1 <-createDataPartition(y=df.nomis$Credit.amount,p=0.5,list=FALSE)
train <- df.nomis[train1, ]
test <- df.nomis[-train1, ]
hist(df.nomis$Credit.amount,breaks=50)
# Split dependent variable distribution
hist(train$Credit.amount,breaks=50)
hist(test$Credit.amount,breaks=50)
# fold cross validation
train_control <- trainControl(method="cv", number=10)
fit <- lm(Credit.amount ~ Age,data = df.nomis)
summary(fit)
plot(df.nomis$Age,df.nomis$Credit.amount)
abline(fit)
fit2 <- lm(Credit.amount ~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data = df.nomis)
summary(fit2)
anova(fit,fit2)
# ridge regression
set.seed(42)
model_ridge <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="ridge", metric="RMSE")
model_ridge
set.seed(42)
model_lars <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="lars", metric="RMSE")
model_lars
# Make predictions on the testing dataset
x_test_reg <- df.nomis[,1:6 ]
y_test_reg <- df.nomis[, 7]
prediction_lars <- predict(model_lars,x_test_reg)
RMSE(prediction_lars, y_test_reg)
# decision tree
colnames(df.nomis)
form_reg <- as.formula(
paste0('Credit.amount~',
paste(colnames(train)[1:6],collapse="+")
)
)
form_reg
# Original regression tree
fit_dt_reg
# Complexity correlation
printcp(fit_dt_reg)
plotcp(fit_dt_reg)
# After the cut branches
fit_dt_reg_pruned <- prune(fit_dt_reg,cp =0.024)
print(fit_dt_reg_pruned)
# Variable importance value
fit_dt_reg_pruned$variable.importance
# tree diagram
prp(fit_dt_reg_pruned,
type=1,
extra = 101,
fallen.leaves =TRUE,
main="Decision Tree")
# predicted
# The training set predicts the results
trainpred <- predict(fit_dt_reg_pruned,newdata=train)
# Training set prediction error index
defaultSummary(data.fram(obs = train$Credit.amount,pred=trainpred))
# Training set prediction error index
defaultSummary(data.frame(obs = train$Credit.amount,pred=trainpred))
# Graph the training set predicted results
plot(x=train$Credit.amount,
y=trainpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'training set')
trainlinmod <- lm(trainpred~train$Credit.amount)
abline(trainlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Test set prediction error index
defaultSummary(data.frame(obs= test$Credit.amount,pred=testpred))
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'test set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# tree diagram
prp(fit_dt_reg_pruned,
type=1,
extra = 101,
fallen.leaves =TRUE,
main="Decision Tree")
# predicted
# The training set predicts the results
trainpred <- predict(fit_dt_reg_pruned,newdata=train)
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Variable importance value
fit_dt_reg_pruned$variable.importance
fit_dt_reg$variable.importance
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# feature engineering
# Train/Test split for Regression
set.seed(42)
train1 <-createDataPartition(y=df.nomis$Credit.amount,p=0.7,list=FALSE)
train <- df.nomis[train1, ]
test <- df.nomis[-train1, ]
# Split dependent variable distribution
hist(train$Credit.amount,breaks=50)
hist(test$Credit.amount,breaks=50)
# fold cross validation
train_control <- trainControl(method="cv", number=10)
fit <- lm(Credit.amount ~ Age,data = df.nomis)
summary(fit)
plot(df.nomis$Age,df.nomis$Credit.amount)
abline(fit)
fit2 <- lm(Credit.amount ~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data = df.nomis)
summary(fit2)
anova(fit,fit2)
# ridge regression
set.seed(42)
model_ridge <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="ridge", metric="RMSE")
model_ridge
set.seed(42)
model_lars <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="lars", metric="RMSE")
model_lars
# Make predictions on the testing dataset
x_test_reg <- df.nomis[,1:6 ]
y_test_reg <- df.nomis[, 7]
prediction_lars <- predict(model_lars,x_test_reg)
RMSE(prediction_lars, y_test_reg)
# decision tree
colnames(df.nomis)
form_reg <- as.formula(
paste0('Credit.amount~',
paste(colnames(train)[1:6],collapse="+")
)
)
form_reg
# training model
set.seed(42)
fit_dt_reg <- rpart(
form_reg,#formula
data = train,
method = "anova",# regression
control = rpart.control(cp = 0.005)
)
# Original regression tree
fit_dt_reg
# Complexity correlation
printcp(fit_dt_reg)
plotcp(fit_dt_reg)
# After the cut branches
fit_dt_reg_pruned <- prune(fit_dt_reg,cp =0.024)
print(fit_dt_reg_pruned)
# Variable importance diagram
varimpdata <-
data.frame(importance=fit_dt_reg_pruned$variable.importance)
ggplot(varimpdata,
aes(x = as_factor(rownames(varimpdata)),y=importance))+
geom_col()+
labs(x = 'variables')+
theme_classic()
# tree diagram
prp(fit_dt_reg_pruned,
type=1,
extra = 101,
fallen.leaves =TRUE,
main="Decision Tree")
# predicted
# The training set predicts the results
trainpred <- predict(fit_dt_reg_pruned,newdata=train)
# Training set prediction error index
defaultSummary(data.frame(obs = train$Credit.amount,pred=trainpred))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Test set prediction error index
defaultSummary(data.frame(obs= test$Credit.amount,pred=testpred))
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'test set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Training set and prediction set result set presentation
predresult <-
data.frame(obs = c(train$Checking.account,test$Checking.account),
pred = c(trainpred,testpred),
group = c (rep("Train",length(trainpred)),
group = c(rep("Train"),length(trainpred)),
rep("Test",length(testpred))))
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'test set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Test set prediction error index
defaultSummary(data.frame(obs= test$Credit.amount,pred=testpred))
# feature engineering
# Train/Test split for Regression
set.seed(42)
train1 <-createDataPartition(y=df.nomis$Credit.amount,p=0.5,list=FALSE)
train <- df.nomis[train1, ]
test <- df.nomis[-train1, ]
hist(df.nomis$Credit.amount,breaks=50)
# Split dependent variable distribution
hist(train$Credit.amount,breaks=50)
hist(test$Credit.amount,breaks=50)
# fold cross validation
train_control <- trainControl(method="cv", number=10)
fit <- lm(Credit.amount ~ Age,data = df.nomis)
summary(fit)
plot(df.nomis$Age,df.nomis$Credit.amount)
abline(fit)
fit2 <- lm(Credit.amount ~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data = df.nomis)
summary(fit2)
anova(fit,fit2)
set.seed(42)
model_lars <- train(Credit.amount~ Age+Sex+Job+Housing+Saving.accounts+Checking.account,data= train, trControl=train_control,method="lars", metric="RMSE")
model_lars
# decision tree
colnames(df.nomis)
form_reg <- as.formula(
paste0('Credit.amount~',
paste(colnames(train)[1:6],collapse="+")
)
)
form_reg
# training model
set.seed(42)
fit_dt_reg <- rpart(
form_reg,#formula
data = train,
method = "anova",# regression
control = rpart.control(cp = 0.005)
)
# Original regression tree
fit_dt_reg
# Complexity correlation
printcp(fit_dt_reg)
plotcp(fit_dt_reg)
# After the cut branches
fit_dt_reg_pruned <- prune(fit_dt_reg,cp =0.024)
print(fit_dt_reg_pruned)
# Variable importance diagram
varimpdata <-
data.frame(importance=fit_dt_reg_pruned$variable.importance)
ggplot(varimpdata,
aes(x = as_factor(rownames(varimpdata)),y=importance))+
geom_col()+
labs(x = 'variables')+
theme_classic()
# tree diagram
prp(fit_dt_reg_pruned,
type=1,
extra = 101,
fallen.leaves =TRUE,
main="Decision Tree")
# predicted
# The training set predicts the results
trainpred <- predict(fit_dt_reg_pruned,newdata=train)
# Training set prediction error index
defaultSummary(data.frame(obs = train$Credit.amount,pred=trainpred))
# Graph the training set predicted results
plot(x=train$Credit.amount,
y=trainpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'training set')
trainlinmod <- lm(trainpred~train$Credit.amount)
abline(trainlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
# Test sets predict results
testpred <- predict(fit_dt_reg_pruned,newdata = test)
# Test set prediction error index
defaultSummary(data.frame(obs= test$Credit.amount,pred=testpred))
# Graph the test set predicted results
plot(x=test$Credit.amount,
y=testpred,
xlab="Actual",
ylab="Prediction",
main='Comparison of actual and predicted values',
sub= 'test set')
testlinmod <- lm(testpred~test$Credit.amount)
abline(testlinmod,col="blue",lwd = 2.5,lty='solid')
abline(a=0,b=1,col='red',lwd=2.5,lty='dashed')
legend("topleft",
legend = c("Model","Base"),
col  = c("blue","red"),
lwd = 2.5,
lty = c("solid","dashed"))
